{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61916f4b",
   "metadata": {},
   "source": [
    "Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f7222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8decb02",
   "metadata": {},
   "source": [
    "Scrape Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3ca8002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "response1 = requests.get(url)\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(response1.content, 'html.parser')\n",
    "soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fd48ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quotes to Scrape'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title1 = soup1.title.text\n",
    "title1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = soup1.find_all('div', class_ = 'quote')\n",
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9034cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_list = []\n",
    "for quote in quotes:\n",
    "    text = quote.find(\"span\", class_ = 'text').text\n",
    "    author = quote.find(\"small\", class_ = 'author').text\n",
    "    tag = [tag.text for tag in quote.find_all('a', class_ = 'tag')]\n",
    "\n",
    "    quotes_list.append({\n",
    "        'text':text,\n",
    "        'author':author,\n",
    "        'tag':tag\n",
    "    }\n",
    "    )\n",
    "quotes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178deeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has beeen written successfully to quotes.csv\n"
     ]
    }
   ],
   "source": [
    "with open('Quotes.csv','w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['quotes','author','tag'])\n",
    "    for quote in quotes_list:\n",
    "        writer.writerow([quote['text'], quote['author'], \",\".join(quote['tag'])])\n",
    "\n",
    "print(\"Data has beeen written successfully to quotes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9901a",
   "metadata": {},
   "source": [
    "Multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "quotes_list = []\n",
    "for page in range(1, 6): \n",
    "    url = f\"https://quotes.toscrape.com/page/{page}/\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    quotes = soup.find_all('div', class_ = 'quote')\n",
    "\n",
    "    for quote in quotes:\n",
    "        text = quote.find(\"span\", class_ = 'text').text\n",
    "        author = quote.find(\"small\", class_ = 'author').text\n",
    "        tag = [tag.text for tag in quote.find_all('a', class_ = 'tag')]\n",
    "\n",
    "        quotes_list.append({\n",
    "            'text':text,\n",
    "            'author':author,\n",
    "            'tag':tag\n",
    "        }\n",
    "        )\n",
    "    time.sleep(2)\n",
    "\n",
    "with open('Quotes2.csv','w',newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['quotes','author','tag'])\n",
    "    for quote in quotes_list:\n",
    "        writer.writerow([quote['text'], quote['author'], \",\".join(quote['tag'])])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552dc06",
   "metadata": {},
   "source": [
    "Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9837c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "News_url = \"https://commoncrawl.org/\"\n",
    "News_response = requests.get(News_url)\n",
    "News_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1342bf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geolocated dataset of German news articles\n",
      "Summary:Geolocating and embedding 50M German news articles for semantic analysis\n",
      "\n",
      "Web Crawl Refusals: Insights From Common Crawl\n",
      "Summary:A study on web crawlers facing inconsistent and poorly-signalled blocking\n",
      "\n",
      "Banned Books: Analysis of Censorship on Amazon.com\n",
      "Summary:Research on Free Expression Online\n",
      "\n",
      "Harmony in the Australian Domain Space\n",
      "Summary:Analyzing the Australian Web with Web Graphs: Harmonic Centrality at the Domain Level\n",
      "\n",
      "Hyperlink Hijacking: Exploiting Erroneous URL Links to Phantom Domains\n",
      "Summary:The Dangers of Hijacked Hyperlinks\n",
      "\n",
      "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "Summary:Enhancing Computational Analysis\n",
      "\n",
      "esCorpius: A Massive Spanish Crawling Corpus\n",
      "Summary:Computation and Language\n",
      "\n",
      " BacklinkDB: A Purpose-Built Backlink Database Management System\n",
      "Summary:The Web as a Graph (Master's Thesis)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "News_soup = BeautifulSoup(News_response.text, 'html.parser')\n",
    "Headlines = News_soup.find_all(\"h3\", class_ = 'heading-m')\n",
    "Summaries = News_soup.find_all(\"div\", class_ = 'heading-ss')\n",
    "\n",
    "Headlines_List = []\n",
    "Summaries_List = []\n",
    "for Headline in Headlines:\n",
    "    Headline_text = Headline.text\n",
    "    Headlines_List.append(Headline_text)\n",
    "for Summary in Summaries:\n",
    "    Summary_text = Summary.text\n",
    "    Summaries_List.append(Summary_text)\n",
    "\n",
    "for i in range(len(Summaries_List)):\n",
    "    print(f\"{Headlines_List[i]}\\nSummary:{Summaries_List[i]}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
